\section{Monte Carlo EM Procedure}

Let $X_{ij} = [x_i, x_j, x_{ij}]$ denote the features, $\Delta_{ij} = [\alpha_i, \beta_j, \gamma_i, u_i, v_j, s_i]$ denote the latent factors, and
$\Theta = [b, g_0, d_0,, c_0, G, D, H, \lambda, \eta, a_\alpha, a_\beta, a_\gamma, A_u, A_v, A_s]$ denote the model parameters. We use the convention that $y = \{y_{ij}\}$, $X = \{X_{ij}\}$, $z = \{z_{jn}\}$ and so on. The EM algorithm seeks to find the $\Theta$ that maximizes the incomplete data likelihood (marginalizing over latent factors):
$$
\arg\max_{\Theta} \Pr[y, w \,|\, \Theta, X]
$$
Let $LL(\Theta; \Delta, z, y, w, X) = \log(\Pr[\Delta, z, y, w \,|\, \Theta, X])$ denote the log of the complete data likelihood. Let $\hat{\Theta}^{(t)}$ denote our current estimate of $\Theta$ at the $t$th iteration. The EM algorithm iterate through the following two steps until convergence.

\begin{itemize}
\item {\bf E-step:} Compute the sufficient statistics of $E_{\Delta, z}[LL(\Theta; \Delta, z, y, w, X) \,|\, \hat{\Theta}^{(t)}]$ as a function of $\Theta$, where the expectation is taken over the posterior distribution of $(\Delta, z \,|\, \hat{\Theta}^{(t)}, y, w, X)$.

\item {\bf M-step:} Find the $\Theta$ that maximizes the expectation computed in the E-step.
$$
\hat{\Theta}^{(t+1)}= \arg\max_{\Theta}
 E_{\Delta, z}[LL(\Theta; \Delta, z, y, w, X) \,|\, \hat{\Theta}^{(t)}]
$$
\end{itemize}

\subsection{Monte-Carlo E-Step}

Because $E_{\Delta, z}[LL(\Theta; \Delta, z, y, w, X) \,|\, \hat{\Theta}^{(t)}]$ is not in closed form, we compute the Monte-Carlo mean based on $L$ samples generated by a Gibbs sampler. The Gibbs sampler repeats the following procedure for $L$ times.

\begin{enumerate}
\item Sample from $(\alpha_i \,|\, \textrm{Rest})$, which is Gaussian, for all $i$.
\begin{equation}
\begin{split}
\mbox{Let } o_{ij} & = y_{ij} - (x_{ij}^{\prime} b)\gamma_i - \beta_j - 
	u_i^\prime v_j - s_{i}^{\prime} \bar{z}_j \\
\textit{Var}[\alpha_i|\mbox{Rest}] & = 
	( \frac{1}{a_{\alpha}} + 
		\sum_{j \in \mathcal{J}_i} \frac{1}{\sigma_{ij}^{2}} )^{-1} \\
E[\alpha_i|\mbox{Rest}] & = 
	\textit{Var}[\alpha_i|\mbox{Rest}]
	( \frac{g_0^{\prime} x_i}{a_{\alpha}} + 
		 \sum_{j \in \mathcal{J}_i} \frac{o_{ij}}{\sigma_{ij}^{2}} )
\end{split}
\end{equation}

\item Sample from $(\beta_j \,|\, \textrm{Rest})$, which is Gaussian, for all $j$.
\begin{equation}
\begin{split}
\mbox{Let } o_{ij} & = y_{ij} - (x_{ij}^{\prime} b) \gamma_i - \alpha_i - 
	u_i^\prime v_j - s_{i}^{\prime} \bar{z}_j \\
\textit{Var}[\beta_j|\mbox{Rest}] & = 
	( \frac{1}{a_{\beta}} + 
		\sum_{i \in \mathcal{I}_j} \frac{1}{\sigma_{ij}^{2}} )^{-1} \\
E[\beta_j|\mbox{Rest}] & =  
	\textit{Var}[\beta_j|\mbox{Rest}]
	( \frac{d_0^{\prime} x_j}{a_{\beta}} + 
		 \sum_{i \in \mathcal{I}_j} \frac{o_{ij}}{\sigma_{ij}^{2}} )
\end{split}
\end{equation}

\item Sample from $(\gamma_i \,|\, \textrm{Rest})$, which is Gaussian, for all $i$.
\begin{equation}
\begin{split}
\mbox{Let } o_{ij} & =  y_{ij} - \alpha_i - \beta_j - u_i^\prime v_j -
	s_i^\prime \bar{z}_j \\
\textit{Var}[\gamma_i|\mbox{Rest}] & =  
	( \frac{1}{a_{\gamma}} + 
	  \sum_{j \in \mathcal{J}_i} 
			\frac{(x_{ij}^{\prime} b)^2}{\sigma_{ij}^{2}} 
	)^{-1} \\
E[\gamma_i|\mbox{Rest}] & = 
	\textit{Var}[\gamma_i|\mbox{Rest}]
	( \frac{ c_0^\prime x_i }{a_{\gamma}} + 
		 \sum_{j \in \mathcal{J}_i} 
			\frac{o_{ij} (x_{ij}^{\prime} b)}{\sigma_{ij}^{2}} )
\end{split}
\end{equation}

\item Sample from $(u_i \,|\, \textrm{Rest})$, which is Gaussian, for all $i$.
\begin{equation}
\begin{split}
\mbox{Let } o_{ij} & =  y_{ij} - (x_{ij}^{\prime} b)\gamma_i - 
						\alpha_i - \beta_j - s_i^\prime \bar{z}_j \\
\textit{Var}[u_i|\mbox{Rest}] & =  
	( A_u^{-1} + 
	  \sum_{j \in \mathcal{J}_i} 
			\frac{v_j v_j^{\prime}}{\sigma_{ij}^{2}} 
	)^{-1} \\
E[u_i|\mbox{Rest}] & = 
	\textit{Var}[u_i|\mbox{Rest}]
	( A_u^{-1} G x_i + 
		 \sum_{j \in \mathcal{J}_i} \frac{o_{ij} v_j}{\sigma_{ij}^{2}} )
\end{split}
\end{equation}

\item Sample from $(v_j \,|\, \textrm{Rest})$, which is Gaussian, for all $j$.
\begin{equation}
\begin{split}
\mbox{Let } o_{ij} & =  y_{ij} - (x_{ij}^{\prime} b)\gamma_i - 
						\alpha_i - \beta_j - s_i^\prime \bar{z}_j \\
\textit{Var}[v_j|\mbox{Rest}] & =  
	( A_v^{-1} + 
	  \sum_{i \in \mathcal{I}_j} 
			\frac{u_i u_i^{\prime}}{\sigma_{ij}^{2}} 
	)^{-1} \\
E[v_j|\mbox{Rest}] & = 
	\textit{Var}[v_j|\mbox{Rest}]
	( A_v^{-1} D x_j + 
		 \sum_{i \in \mathcal{I}_j} \frac{o_{ij} u_i}{\sigma_{ij}^{2}} )
\end{split}
\end{equation}

\item Sample from $(s_i \,|\, \textrm{Rest})$, which is Gaussian, for all $i$.
\begin{equation}
\begin{split}
\mbox{Let } o_{ij} & =  y_{ij} - (x_{ij}^{\prime} b)\gamma_i - 
						\alpha_i - \beta_j - u_i^\prime v_j \\
\textit{Var}[s_i|\mbox{Rest}] & =  
	( A_s^{-1} + 
	  \sum_{j \in \mathcal{J}_i} 
			\frac{\bar{z}_j \bar{z}_j^{\prime}}{\sigma_{ij}^{2}} 
	)^{-1} \\
E[s_i|\mbox{Rest}] & = 
	\textit{Var}[s_i|\mbox{Rest}]
	( A_s^{-1} H x_i + 
		 \sum_{j \in \mathcal{J}_i} \frac{o_{ij} \bar{z}_j}{\sigma_{ij}^{2}} )
\end{split}
\end{equation}

\item Sample from $(z_{jn} \,|\, \textrm{Rest})$, which is multinomial, for all $j$ and $n$. Let $z_{\neg jn}$ denote $z$ with $z_{jn}$ removed. The probability of $z_{jn}$ being topic $k$ is given by
\begin{equation*}
\begin{split}
&\Pr[z_{jn} = k \,|\, z_{\neg jn}, \Delta, \hat{\Theta}^{(t)}, y, w, X] \\
&\propto \Pr[z_{jn} = k, y \,|\, z_{\neg jn}, \Delta, \hat{\Theta}^{(t)}, w, X] \\
&\propto \Pr[z_{jn} = k \,|\, w, z_{\neg jn}, \hat{\Theta}^{(t)}] \cdot 
  		  \prod_{i\in I_j} \Pr[y_{ij}\,|\,z_{jn} = k, z_{\neg jn}, 
								   \Delta, \hat{\Theta}^{(t)}, X]
\end{split}
\end{equation*}
Let
$$
Z_{jk\ell} = \sum_n \mathbf{1}\{z_{jn} = k \mbox{ and } w_{jn} = \ell\}
$$
denote the number of times word $\ell$ belongs to topic $k$ in item $j$. Let $Z_{k\ell} = \sum_j Z_{jk\ell}$, $Z_{k} = \sum_{j\ell} Z_{jk\ell}$, and so on. We use $Z_{\cdot}^{\neg jn}$ to denote the count $Z_{\cdot}$ with $z_{jn}$ removed from the summation. Assume $w_{jn} = \ell$. Then,
\begin{equation*}
\begin{split}
& \Pr[z_{jn} = k \,|\, w, z_{\neg jn}, \hat{\Theta}^{(t)}] \\
&	\propto \Pr[z_{jn} = k, w_{jn} = \ell \,|\, 
					w_{\neg jn}, z_{\neg jn}, \hat{\Theta}^{(t)}] \\
&	= \Pr[w_{jn} = \ell \,|\, 
			w_{\neg jn}, z_{jn} = k, z_{\neg jn}, \eta] ~
	  \Pr[z_{jn} = k \,|\, z_{\neg jn}, \lambda] \\
&	= E[ \Phi_{k\ell} \,|\, w_{\neg jn}, z_{\neg jn}, \eta] ~
	  E[ \theta_{jk} \,|\, z_{\neg jn}, \lambda] \\
& 	= \frac{Z_{k\ell}^{\neg jn} + \eta}
			 {Z_{k}^{\neg jn} + W \eta}~
	  \frac{Z_{jk}^{\neg jn} + \lambda_k}
			 {Z_{j}^{\neg jn} + \sum_k \lambda_k}
\end{split}
\end{equation*}
Note that the denominator of the second term $(Z_{j}^{\neg jn} + \sum_k \lambda_k)$ is independent of $k$. Thus, we obtain
\begin{equation}
\Pr[z_{jn} = k \,|\, \mbox{Rest}]
 \propto \frac{Z_{k\ell}^{\neg jn} + \eta}
			 {Z_{k}^{\neg jn} + W \eta} ~ 
			 (Z_{jk}^{\neg jn} + \lambda_k) ~
			 \prod_{i\in \mathcal{I}_j} f_{ij}(y_{ij})
\end{equation}
where $f_{ij}(y_{ij})$ is the probability density at $y_{ij}$ given the current values of $(x_{ij}^{\prime}\, b) \gamma_i  + \alpha_i + \beta_j + u_i^\prime v_j + s_{i}^{\prime} \, \bar{z}_{j}$ and $\sigma_{ij}^2$.
\begin{equation}
\begin{split}
\mbox{Let } o_{ij} 
	&= y_{ij} - (x_{ij}^{\prime}\, b) \gamma_i  - 
		\alpha_i - \beta_j - u_i^\prime v_j \\
\prod_{i\in \mathcal{I}_j} f_{ij}(y_{ij}) 
	&\propto \exp\left\{ 
			-\frac{1}{2} \sum_{i \in \mathcal{I}_j}
				\frac{(o_{ij} -  s_{i}^{\prime} \, \bar{z}_{j})^2}{\sigma_{ij}^2}
		\right\} \\
	&\propto \exp\left\{
			\bar{z}_{j}^\prime B_j - 
			\frac{1}{2} \bar{z}_{j}^\prime C_j \bar{z}_{j}
		\right\} \\
\mbox{where } & 
	B_j = \sum_{i \in \mathcal{I}_j} \frac{o_{ij} s_i}{\sigma_{ij}^{2}}
	\mbox{ and }
	C_j = \sum_{i \in \mathcal{I}_j} \frac{s_i s_i^{\prime}}{\sigma_{ij}^{2}}
\end{split}
\end{equation}
\end{enumerate}

\subsection{M-Step}

In the M-step, we want to find the $\Theta$ = $[b, g_0, c_0, d_0, G, D, H, \lambda, \eta, a_\alpha, a_\beta, a_\gamma, A_u, A_v]$ that maximizes the expected complete data likelihood computed in the E-step.
$$
\hat{\Theta}^{(t+1)}= \arg\max_{\Theta}
 E_{\Delta, z}[LL(\Theta; \Delta, z, y, w, X) \,|\, \hat{\Theta}^{(t)}]
$$
where $- LL(\Theta; \Delta, z, y, w, X) =$
{\small\begin{equation*}
\begin{split}
& ~~   \mbox{Constant} +
			\frac{1}{2} \sum_{ij} \left( \frac{1}{\sigma_{ij}^{2}}
			(y_{ij}-\alpha_i-\beta_j- (x_{ij}^{\prime}b) \gamma_i
				- u_i^\prime v_j - s_{i}^{\prime} \bar{z}_{j})^{2}
			+ \log \sigma_{ij}^{2} \right) \\
& ~~ + \frac{1}{2 a_{\alpha}} \sum_{i}
				(\alpha_i - g_{0}^{\prime}x_i)^{2}
				+ \frac{M}{2} \log a_{\alpha}
		+ \frac{1}{2 a_{\gamma}} \sum_{i} 
				(\gamma_i - c_{0}^{\prime}x_i)^{2}
				+ \frac{M}{2} \log a_{\gamma}  \\
& ~~	+ \frac{1}{2} \sum_{i} 
				(u_i - Gx_i)^{\prime} A_u^{-1} (u_i - Gx_i)
				+ \frac{M}{2} \log(\det A_u) \\
& ~~	+ \frac{1}{2} \sum_{j} 
				(v_j - Dx_j)^{\prime} A_v^{-1} (v_j - Dx_j)
				+ \frac{N}{2} \log(\det A_v) \\
& ~~	+ \frac{1}{2} \sum_{i} 
				(s_i - Hx_i)^{\prime} A_s^{-1} (s_i - Hx_i)
				+ \frac{M}{2} \log(\det A_s) \\
& ~~ 	+ \frac{1}{2 a_{\beta}} \sum_{j}  
				 (\beta_j - d_{0}^{\prime}x_j)^{2}
				+ \frac{N}{2} \log a_{\beta}  \\
& ~~ 	+ N \left( \sum_k \log\Gamma(\lambda_k) 
				- \log\Gamma(\textstyle\sum_k \lambda_k) \right)
				+ \sum_j \left(
						\log\Gamma\left(Z_j + \textstyle\sum_k \lambda_k\right) -
						\sum_k \log\Gamma(Z_{jk} + \lambda_k)
				  \right) \\
& ~~ 	+ K \left( W \log\Gamma(\eta) 
				- \log\Gamma(W \eta) \right)
				+ \sum_k \left(
						\log\Gamma(Z_k + W \eta) -
						\sum_\ell \log\Gamma(Z_{k\ell} + \eta)
				  \right) \\
\end{split}
\end{equation*}}

\noindent The optimal $g_0$, $c_0$, $G$, $D$, $d_0$, $a_\alpha$, $a_\gamma$, $A_u$, $A_v$ and $a_\beta$ can be obtained using the same regression procedure as that in the original RLFM.
\\

{\bf Optimal $b$ and $\sigma^2$:} Consider the Gaussian case, where $\sigma_{ij}^2 = \sigma^2$. Define
$$
o_{ij} = y_{ij} - (\alpha_i + \beta_j + u_i^\prime v_j + s_i^\prime \bar{z}_j)
$$
We use $\tilde{E}[\cdot]$ to denote the Monte-Carlo expectation. Here, we want to find $b$ and $\sigma^2$ that minimize
\begin{equation*}
\begin{split}
&	\frac{1}{\sigma^2} \sum_{ij} 
		\tilde{E}[(o_{ij} - \gamma_i(x_{ij}^{\prime}b))^2]
		+ P \log(\sigma^2) \\
&=	\frac{1}{\sigma^2} \sum_{ij} \left(
			\tilde{E}[o_{ij}^2] 
			- 2\tilde{E}[o_{ij}\gamma_i] (x_{ij}^{\prime}b)
			+ \tilde{E}[\gamma_i^2] (x_{ij}^{\prime}b)^2
		\right)
		+ P \log(\sigma^2) \\
&=	\frac{1}{\sigma^2} \sum_{ij} \left(
			\tilde{E}[o_{ij}^2]
			- \frac{(\tilde{E}[o_{ij}\gamma_i])^2}{\tilde{E}[\gamma_i^2]}
			+ \tilde{E}[\gamma_i^2] \left(
					\frac{\tilde{E}[o_{ij}\gamma_i]}{\tilde{E}[\gamma_i^2]}
					- x_{ij}^{\prime} b
				\right)^2
		\right)
		+ P \log(\sigma^2) \\
\end{split}
\end{equation*}
where $P$ is the total number of observed user-item pairs. The optimal $b$ can be found by weighted least squares regression with feature $x_{ij}$, response $\frac{\tilde{E}[o_{ij}\gamma_i]}{\tilde{E}[\gamma_i^2]}$ and weight $\tilde{E}[\gamma_i^2]$. The optimal $\sigma^2$ is the above summation (over $ij$) divided by $P$ with the optimal $b$ value.
\\

{\bf Optimal $\eta$:} We want to find $\eta$ that minimizes
\begin{equation*}
\begin{split}
& K \left( W \log\Gamma(\eta) 
				- \log\Gamma(W \eta) \right)
				+ \sum_k \left(
						\tilde{E}[\log\Gamma(Z_k + W \eta)] -
						\sum_\ell \tilde{E}[\log\Gamma(Z_{k\ell} + \eta)]
				  \right) \\
\end{split}
\end{equation*}
Since this optimization is just one dimensional and $\eta$ is a nuance parameter, we can simply try a number of fixed possible $\eta$ values.
\\

{\bf Optimal $\lambda$:} We want to find $\lambda_1$, ..., $\lambda_K$ that minimize
\begin{equation*}
\begin{split}
& N \left( \sum_k \log\Gamma(\lambda_k) 
		- \log\Gamma(\textstyle\sum_k \lambda_k) \right) \\
&		+ \sum_j \left(
				\tilde{E}[\log\Gamma\left(Z_j + \textstyle\sum_k \lambda_k\right)]
				- \sum_k \tilde{E}[\log\Gamma(Z_{jk} + \lambda_k)]
	\right)\\
\end{split}
\end{equation*}
{\bf Question: How to optimize this efficiently?} What's the sufficient statistics? Storing Monte-Carlo samples in memory for $Z_{jk}$ may not be feasible. Any approximation? For now, I just assume $\lambda_k = \lambda_0$, a single scalar and search through a fixed set of points (which is also assumed in the first Gibbs-sampling-based LDA paper by Griffiths, 2004).

{\bf Comment:}: I think assuming the same $\lambda$ should work fine with a lot of document data, in fact we can select $\eta$ and $\lambda_0$ through cross-validation in the first  implementation to simplify things.

{\bf Comment:} The Gibbs sampler here looks trivial, I also feel it would be better behaved than the sampler we had in RLFM. However, one drawback is the conditional
distribution of $u_i$ that involves inverting a matrix whose dimension equals number of topics. In general, we may want to try to a few thousand topics with LDA; we may have to break $u_i$ into blocks of small chunks and sample each block one at a time conditional on the others. However, this does not seem to be a worry in the first implementation where we could try few 100 topics in the LDA.
