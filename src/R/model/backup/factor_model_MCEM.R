### Copyright (c) 2011, Yahoo! Inc.  All rights reserved.
### Copyrights licensed under the New BSD License. See the accompanying LICENSE file for terms.
### 
### Author: Bee-Chung Chen

###
### The simple random init
###	  alpha, beta, v are generated by N(0,sd)
###   b fitted with lm 
###   g0, d0, G are all 0
###
init.simple_random <- function(
	obs, feature, nFactors,
	is.logistic=FALSE,
	var_alpha=1, var_beta=1, var_v=1, var_y=NULL,
	sd.multiplier=0.1
){
	nObs = nrow(obs); nUsers = nrow(feature$x_user); nItems = nrow(feature$x_item);
	nUserFeatures = ncol(feature$x_user);  nItemFeatures = ncol(feature$x_item);
	
	if(is.logistic){
		var_y = 1; draw.sd = 1;
		b = rep(0, ncol(feature$x_dyad));
		y.mean = mean(obs$y);
		bias = log(y.mean / (1-y.mean));
		if(all(feature$x_dyad[,1] == 1)){
			b[1] = bias;
		}else if(all(feature$x_dyad[,ncol(feature$x_dyad)] == 1)){
			b[ncol(feature$x_dyad)] = bias;
		}
	}else{
		if(all(feature$x_dyad == 0)) fit = list(fitted.values=0, coefficients=rep(0,ncol(feature$x_dyad)))
		else                         fit = lm(obs$y ~ feature$x_dyad - 1, model=FALSE);
		if(length(fit$coef) != ncol(feature$x_dyad)) stop("length(fit$coef) != ncol(feature$x_dyad)");
		rss = sum((fit$fitted.values - obs$y)^2);
		
		b = fit$coefficients;
		if(is.null(var_y)) var_y = rss / nObs;
		
		draw.sd = sqrt(var_y) * sd.multiplier;
	}
	
	factor = list(alpha=rnorm(nUsers,sd=draw.sd),beta=rnorm(nUsers,sd=draw.sd),
			v=matrix(rnorm(nUsers*nFactors,sd=draw.sd),nrow=nUsers));
	
	param  = list(b=b, g0=rep(0,nUserFeatures), d0=rep(0,nUserFeatures), 
			G=matrix(rep(0,nFactors*nUserFeatures),nrow=nUserFeatures),
			var_y=var_y, var_alpha=var_alpha, var_beta=var_beta, var_v=var_v);

	if(is.logistic) param$xi = rep(1.0, nrow(obs));
	
	if(nFactors == 0){
		param$var_v = NULL;
	}
	
	ans = list(factor=factor, param=param);
}

###
### Monte-Carlo EM (through Gibbs sampling)
###
### Initialization of parameters should be done before calling this method.
###
###  MODEL:  y_{ij} ~ N(x_dyad_{ij} b + alpha_i + beta_j + v_i v_j,  var_y)
###			alpha_i ~ N(g0 x_user_i,  var_alpha)
###			 beta_j ~ N(d0 x_user_j,  var_beta)
###				v_i ~ N( G x_user_i,  var_v)
###
###  INPUT: Training Dataset
###         factor  = list(alpha, beta, v); # Initial factor values
###         obs     = data.frame(author, voter, item, y);
###         feature = list(x_dyad, x_user, x_item);
###         param   = list(b, g0, d0, G,
###                        var_y, var_alpha, var_beta, var_v);
###         
###         IMPORTANT NOTE: In the training dataset, every user and item has to have observed
###         rating and features; otherwise, you will receive an error message.
###			In the test data, it is fine to have new users and items.
###
###         Test Dataset (OPTIONAL: used to compute test-set metrics over iterations)
###         Test dataset may include new items and new users
###         test.obs     = data.frame(author, voter, item, y);
###         test.feature = list(x_dyad, x_user, x_item);
###
###   NOTE: (1) If you want an INTERCEPT in regression, please add the intercept column in
###             feature${x_dyad,x_user,x_item} by yourself.
###         (2) To DISABLE fitting a FACTOR, set param$var_FACTOR == NULL. Both sampling for the
###             FACTOR and regression for the FACTOR will be disabled.
###
### OUTPUT:
###   output$est.last:    Parameter estimates at the end of the last iteration
###   output$est.minTestLoss: Parameter estimates with the lowest test-set loss computed using the test dataset
###                           if is.null(test.obs), then this output will NOT be available.
###
###	is.logistic=TRUE still needs validation!!!
###
fit.MCEM <- function(
		nIter,        # Number of EM iterations
		nSamples,     # Number of samples drawn in each E-step: could be a vector of size nIter.
		nBurnIn,      # Number of burn-in draws before take samples for the E-step: could be a vector of size nIter.
		factor,       # Initial factor values
		obs,          # Dyadic observations
		feature,      # Feature values
		param,        # Initial parameter values
		test.obs=NULL,     # Test data: Dyadic observations for testing
		test.feature=NULL, #            Feature values for testing
		userIDs=NULL,
		itemIDs=NULL,
		is.logistic=FALSE,
		out.level=0,  # out.level=1: Save the factor & parameter values to out.dir/est.last and out.dir/est.minTestLoss
		out.dir=NULL, # out.level=2: Save the factor & parameter values of each iteration i to out.dir/est.i
		out.append=FALSE,
		debug=0,      # Set to 0 to disable internal sanity checking; Set to 100 for most detailed sanity checking
		verbose=0,    # Set to 0 to disable console output; Set to 100 to print everything to the console
		verbose.E=verbose-1,
		verbose.M=verbose-1,
		use.C=TRUE,   # Whether to use the C implementation (R implementation does not have full functionalities)
		lm=F,         # Whether to use lm to fit linear regression
		fit.var_v,    # Whether to fit var_v
		is.G.zero, 	  # Set G  to zero?
		is.d0.zero,   # Set d0 to zero?
		error.handler=stop, # You can change it to warning so that the program won't stop
		output.at.end.of.EStep=FALSE,
		rm.factors.without.obs.in.loglik=TRUE,
		...           # Additional parameters passing into the regression functions (e.g., bayesglm)
){
	obs$author = as.integer(obs$author);
	obs$voter  = as.integer(obs$voter);
	
	if(any(obs$author == obs$voter)) stop("self vote is not allowed");
	
	obs      = init.obs(obs=obs,      is.logistic=is.logistic);
	test.obs = init.obs(obs=test.obs, is.logistic=is.logistic);
	
	if(length(nSamples)!=nIter){
		if(length(nSamples) != 1) stop("length(nSamples)!=nIter && length(nSamples) != 1");
		nSamples <- rep(nSamples,nIter);
	}
	if(length(nBurnIn)!=nIter){
		if(length(nBurnIn) != 1) stop("length(nBurnIn)!=nIter && length(nBurnIn) != 1");
		nBurnIn <- rep(nBurnIn,nIter);
	}
	
	factor$rm.factors.without.obs.in.loglik = rm.factors.without.obs.in.loglik;
	
	warning.any.not.in(c("alpha", "beta"), names(factor), "The following components in factor are required: ", stop=TRUE);
	size = syncheck.factorModel.spec(factor=factor, obs=obs, feature=feature, param=param, 
			warning=10, print=TRUE);
	check.Obs(obs, size$nUsers, size$nItems, error=error.handler);
	
	if(!is.null(test.obs)){
		if(is.null(test.feature)) stop("test.obs != NULL, but test.feature is NULL");
	}
	
	factor = deepCopy(factor);
	
	begin.time = proc.time();
	
	LL = rep(NA, nIter+1); # LL records the complete data logLikelihood of each iteration
	ll = logLikelihood(obs=obs, factor=factor, feature=feature, param=param, is.logistic=is.logistic);
	LL[1] = ll;
	
	prediction = NULL;  TestLoss = NULL;  minTestLoss = NULL;  est.minTestLoss = NULL;
	if(!is.null(test.obs)){
		TestLoss = rep(NA, nIter+1); # TestLoss records the loss in the test set of each iteration
		prediction = pred.gauss(fit=list(factor=factor, param=param), obs=test.obs, feature=test.feature, is.logistic=is.logistic);
		minTestLoss = prediction$test.loss;
		TestLoss[1] = minTestLoss;
		# Factor & paramter estimates with min TestLoss
		est.minTestLoss = list(factor=factor, param=param);
	}
	
	time.used = proc.time() - begin.time;
	
	if(verbose >= 1){
		cat("START fit.MCEM (initial complete data logLikelihood: ",ll," + constant)\n",sep="");
		if(!is.null(test.obs))
			cat("                initial test-set loss: ", minTestLoss, "\n",sep="");
	}
	
	output.to.dir(
		out.dir=out.dir, factor=factor, param=param, IDs=list(userIDs=userIDs, itemIDs=itemIDs), 
		prediction=prediction, loglik=ll, 
		minTestLoss=minTestLoss, nSamples=nSamples, iter=0, out.level=out.level, out.append=out.append, 
		TimeEStep=0, TimeMStep=0, TimeTest=time.used[3], verbose=verbose
	);
	
	for(iter in 1:nIter){
		
		response = generate.response(obs=obs, param=param, is.logistic=is.logistic, verbose=verbose);
		obs$y = response$y;
		param$var_y = response$var_y;
		
		if(verbose >= 1){
			cat("---------------------------------------------------------\n",
				"Iteration ",iter,"\n",
				"---------------------------------------------------------\n",
				"start E-STEP\n",sep="");
		}
		b.time = proc.time();
		
		###
		### E-STEP
		###
		if(use.C){
			mc_e = MCEM_EStep.C(
					factor, obs, feature, param, nSamples[iter], nBurnIn=nBurnIn[iter],
					debug=debug, verbose=verbose.E
			);
			# IMPORTANT NOTE: factor is call-by-reference in MCEM_EStep.C
			# Now, factor = mc_e$mean with the same memory address!!!
		}else{
			mc_e = MCEM_EStep.R(
					factor, obs, feature, param, nSamples[iter], nBurnIn=nBurnIn[iter],
					debug=debug, verbose=verbose.E
			);
		}
		factor = mc_e$mean;
		
		time.used.1 = proc.time() - b.time;
		
		# verbose
		if(verbose >= 1 || output.at.end.of.EStep){
			b.time.LL = proc.time();
			ll = logLikelihood(obs=obs, factor=factor, feature=feature, param=param, is.logistic=is.logistic);
			time.used.LL = proc.time() - b.time.LL;
			if(verbose >= 1)
				cat("end   E-STEP (complete data logLikelihood = ",ll," + constant,  used ",time.used.1[3]," sec,  LL used ",time.used.LL[3]," sec)\n", sep="");
			if(!is.null(test.obs) && (verbose >= 2 || output.at.end.of.EStep)){
				b.time.TestLoss = proc.time();
				prediction = pred.gauss(fit=list(factor=factor, param=param), obs=test.obs, feature=test.feature, is.logistic=is.logistic);
				time.used.TestLoss = proc.time() - b.time.TestLoss;
				cat("              test-set loss = ",prediction$test.loss,"  used ",time.used.TestLoss[3]," sec\n",sep="");
			}
		}
		
		if(output.at.end.of.EStep){
			output.to.dir(
				out.dir=out.dir, factor=factor, param=param, IDs=list(userIDs=userIDs, itemIDs=itemIDs), 
				prediction=prediction, loglik=ll, 
				minTestLoss=minTestLoss, nSamples=nSamples, iter=iter, out.level=out.level, out.append=out.append, 
				TimeEStep=time.used.1[3], TimeMStep=0, TimeTest=time.used.TestLoss[3], verbose=verbose,
				other=list(mc_e=mc_e), name="est-end-of-E"
			);
		}
		
		###
		### M-STEP
		###
		b.time = proc.time();
		cat("start M-STEP\n",sep="");
		param.new = MCEM_MStep(
				factor.mean=mc_e$mean, factor.sumvar=mc_e$sumvar, obs=obs, 
				feature=feature, param=param,
				fit.var_v=fit.var_v, is.G.zero=is.G.zero, is.d0.zero=is.d0.zero, 
				debug=0, verbose=verbose.M, lm=lm, ...
		);
		
		param = update.param(factor.mean=mc_e$mean, param=param.new, obs=obs, is.logistic=is.logistic);
		
		time.used.2 = proc.time() - b.time;
		
		b.time.LL = proc.time();
		ll = logLikelihood(obs=obs, factor=factor, feature=feature, param=param, is.logistic=is.logistic);
		LL[iter+1] = ll;
		time.used.LL = proc.time() - b.time.LL;
		
		if(!is.null(test.obs)){
			b.time.TestLoss = proc.time();
			prediction = pred.gauss(fit=list(factor=factor, param=param), obs=test.obs, feature=test.feature, is.logistic=is.logistic);
			TestLoss[iter+1] = prediction$test.loss;
			time.used.TestLoss = proc.time() - b.time.TestLoss;
		}
		time.used.3 = proc.time() - b.time.LL;
		
		if(verbose >= 1){
			cat("end   M-STEP (complete data logLikelihood = ",ll," + constant,  used ",time.used.2[3]," sec,  LL used ",time.used.LL[3]," sec)\n",sep="");
			if(!is.null(test.obs))
				cat("              test-set loss = ",TestLoss[iter+1],"  used ",time.used.TestLoss[3]," sec\n",sep="");
		}
		
		###
		### Update the est.minTestLoss model if the TestLoss decreases
		###
		if(!is.null(test.obs) && TestLoss[iter+1] < minTestLoss){ 
			minTestLoss = TestLoss[iter+1];
			est.minTestLoss$factor=deepCopy(factor);
			est.minTestLoss$param=param;
			
			if(verbose >= 10) cat("TestLoss decreases!\n");
		}
		
		output.to.dir(
			out.dir=out.dir, factor=factor, param=param, IDs=list(userIDs=userIDs, itemIDs=itemIDs), 
			prediction=prediction, loglik=ll, 
			minTestLoss=minTestLoss, nSamples=nSamples, iter=iter, out.level=out.level, out.append=out.append, 
			TimeEStep=time.used.1[3], TimeMStep=time.used.2[3], TimeTest=time.used.3[3], verbose=verbose
		);
	}
	
	output = list(
			trainingCDL     = LL,
			testTestLoss    = TestLoss,
			minTestLoss     = minTestLoss,
			est.minTestLoss = est.minTestLoss,
			est.last = list(factor=factor, param=param)
	);
	
	time.used = proc.time() - begin.time;
	if(verbose >= 1){
		cat("END   fit.MCEM (used ",time.used[3]," sec)\n",sep="");
		if(!is.null(test.obs))
			cat("                min test-set loss: ",minTestLoss,"\n",sep="");
	}
	
	return(output);
}

###
### MCEM_EStep.C. See MCEM_EStep(...) in src/C/factor_model_MC_EStep.c
###
### INPUT:  factor  = list(alpha, beta, v); # Initial factor values
###         obs     = data.frame(author, voter, item, y);
###         feature = list(x_dyad, x_user, x_item);
###         param   = list(b, g0, d0, G,
###                        var_y, var_alpha, var_beta, var_v);
###         
### OUTPUT: mean     = list(alpha, beta, v, fErr);
###         sumvar   = list(alpha, beta, v, fErr);
###         sampvar  = list(alpha, beta, v);
###
###   isOldUser[i]: [TRUE/FALSE] Whehter the ith user is an old user; default: all FALSE
###                 For old users, we set g0x_user[i] = alpha[i], Gx_user[i,,] = v[i,,]
###
### NOTE: factor$alpha and factor$beta cannot be NULL!!
###
MCEM_EStep.C <- function(
		factor, obs, feature, param, nSamples, nBurnIn=1,
		outputFactorVar=FALSE, isOldUser=NULL,
		debug=0, verbose=0
){
	size = syncheck.factorModel.spec(factor=factor, obs=obs, feature=feature, param=param);
	
	if(is.null(factor$alpha)) stop("alpha cannot be null");
	if(is.null(factor$beta))  stop("beta cannot be null");
	if(length(obs) == 0) stop("No observation data");
	
	sumvar  = list(alpha=rep(double(1),1), beta=rep(double(1),1), v=rep(double(1),1), fErr=rep(double(1),1));
	factor$fErr = rep(double(1),size$nObs);
	factor$pred.y.square = rep(double(1),size$nObs);
	sampvar = list();
	if(outputFactorVar){
		sampvar$alpha = rep(double(1),size$nUsers);
		sampvar$beta  = rep(double(1),size$nUsers);
		sampvar$v     = array(double(1),dim=c(size$nUsers, size$nFactors, size$nFactors));
	}
	
	xb  = feature$x_dyad %*% param$b;
	g0x_user = NULL; d0x_user = NULL; Gx_user = NULL;
	if(is.null(isOldUser)){
		if(!is.null(param$g0)) g0x_user = feature$x_user %*% param$g0;
		if(!is.null(param$d0)) d0x_user = feature$x_user %*% param$d0;
		if(!is.null(param$G))   Gx_user = feature$x_user %*% param$G;
	}else{
		if(length(isOldUser) != size$nUsers) stop("length(isOldUser) != nUsers");
		x_user.new = feature$x_user[!isOldUser,,drop=FALSE];
		if(!is.null(param$g0)){ g0x_user = factor$alpha;  g0x_user[!isOldUser] = x_user.new %*% param$g0;}
		if(!is.null(param$d0)){ d0x_user = factor$beta;   d0x_user[!isOldUser] = x_user.new %*% param$d0;}
		if(!is.null(param$G)){   Gx_user = factor$v;      Gx_user[!isOldUser,] = x_user.new %*% param$G;}
	}

	# Checks
	if(size$nVar_alpha > 0 && (!is.double(g0x_user) || length(g0x_user) != size$nUsers)) stop("g0x_user!");
	if(size$nVar_beta  > 0 && (!is.double(d0x_user) || length(d0x_user) != size$nUsers)) stop("d0x_user!");
	if(size$nVar_v     > 0 && (!is.double(Gx_user)  || any(dim(Gx_user) != c(size$nUsers,size$nFactors)))) stop("Gx_user!");
	
	problem.dim = c(size$nObs, size$nUsers, size$nFactors,
					size$nVar_y, size$nVar_alpha, size$nVar_beta, size$nVar_v);
	if(!is.integer(problem.dim)) stop("!is.integer(problem.dim)");
	
	check_type_size(factor$alpha, "double", problem.dim[2]);
	check_type_size(factor$beta, "double", problem.dim[2]);
	check_type_size(factor$v, "double", problem.dim[c(2,3)]);
	check_type_size(sumvar$alpha, "double", 1);
	check_type_size(sumvar$beta, "double", 1);
	check_type_size(sumvar$v, "double", 1);
	check_type_size(sumvar$fErr, "double", 1);
	check_type_size(sampvar$alpha, "double", problem.dim[2], isNullOK=!outputFactorVar);
	check_type_size(sampvar$beta, "double", problem.dim[2], isNullOK=!outputFactorVar);
	check_type_size(sampvar$v, "double", problem.dim[c(2,3,3)], isNullOK=!outputFactorVar);
	check_type_size(obs$voter,  "int", problem.dim[1]);
	check_type_size(obs$author, "int", problem.dim[1]);
	check_type_size(obs$y, "double", problem.dim[1]);
	check_type_size(xb, "double", c(problem.dim[1],1));
	check_type_size(g0x_user, "double", c(problem.dim[2],1), isNullOK=(problem.dim[5]==0));
	check_type_size(d0x_user, "double", c(problem.dim[2],1), isNullOK=(problem.dim[6]==0));
	check_type_size(Gx_user,  "double", problem.dim[c(2,3)], isNullOK=(problem.dim[3]==0 || problem.dim[7]==0));
	check_type_size(param$var_y,  "double", problem.dim[4]);
	check_type_size(param$var_alpha, "double", 1, isNullOK=(problem.dim[5]==0));
	check_type_size(param$var_beta,  "double", 1, isNullOK=(problem.dim[6]==0));
	check_type_size(param$var_v,  "double", 1, isNullOK=(problem.dim[3]==0 || problem.dim[7]==0));
	
	ans = .C("MCEM_EStep",
			# INPUT (initial factor values) & OUTPUT (Monte Carlo mean of factor values)
			factor$alpha, factor$beta, factor$v,
			# OUTPUT
			sumvar$alpha,  	sampvar$alpha,
			sumvar$beta,   	sampvar$beta,
			sumvar$v, 		sampvar$v,
			factor$fErr,	sumvar$fErr,
			factor$pred.y.square,
			# INPUT
			as.integer(nSamples), as.integer(nBurnIn),
			obs$voter, obs$author, obs$y, 
			xb, g0x_user, d0x_user, Gx_user,
			param$var_y, param$var_alpha, param$var_beta, param$var_v,
			problem.dim, as.integer(length(problem.dim)),
			as.integer(if(outputFactorVar) 1 else 0),
			# OTHER
			as.integer(debug), as.integer(verbose),
			DUP=FALSE
	);
	output = list(
			mean=factor, sumvar=sumvar
	);
	if(outputFactorVar) output$sampvar=sampvar;
	return(output);
}

